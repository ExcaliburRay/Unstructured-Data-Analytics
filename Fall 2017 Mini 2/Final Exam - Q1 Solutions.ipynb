{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [95-865] Unstructured Data Analysis: Final Exam Q1\n",
    "\n",
    "Name:  \n",
    "Andrew ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1:  Sentiment Analysis [Total: 25 Points]\n",
    "\n",
    "Universal Brothers, A fictional movie production company approaches you to find out what people feel about the movies the produce.\n",
    "Download the dataset from https://www.dropbox.com/s/1ztjhjsznlhtv10/ExamData-1.zip?dl=0 <br>\n",
    "Unzip into same folder as this notebook. Do not have the files inside any other inner folder.\n",
    "Find attached an IMDB dataset for movie reviews. You will perform sentiment analysis on this dataset. Low rating movies are labeled 0 and high rated movies are labeled 1. You are provided with a train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Load [1 Point]\n",
    "Load the train and test files into a dataframe. Name the columns to help you in the rest of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "train = pd.read_csv( os.path.join('train.csv'))\n",
    "test = pd.read_csv(os.path.join('test.csv'))\n",
    "train = train[train.columns[[1,2]]]\n",
    "train.columns = ['Sentiment', 'Review']\n",
    "test = test[test.columns[[1,2]]]\n",
    "test.columns = ['Sentiment', 'Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Clean the Dataset [4 points]\n",
    "\n",
    "- Tokenize the reviews using Spacy\n",
    "- Convert tokens to lower case\n",
    "- Only keep alphanumeric characters in the reviews\n",
    "- Use the stopwords.txt file to remove stop words from the tokens list\n",
    "- Remove punctuations from the reviews\n",
    "- Perform the above on both the train and test review datasets\n",
    "- List the tokens for the first 5 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>[classic, war, timothy, hines, entertaining, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>[assumed, praised, film, greatest, filmed, ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                             Review  \\\n",
       "0          1  With all this stuff going down at the moment w...   \n",
       "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3          0  It must be assumed that those who praised this...   \n",
       "4          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [stuff, going, moment, mj, started, listening,...  \n",
       "1  [classic, war, timothy, hines, entertaining, f...  \n",
       "2  [film, starts, manager, nicholas, bell, giving...  \n",
       "3  [assumed, praised, film, greatest, filmed, ope...  \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def tokenize(review):\n",
    "        review = review.lower()\n",
    "        review = list(nlp(review.decode(\"latin-1\").strip()))\n",
    "        review = [str(x) for x in review]\n",
    "        tokens = [x for x in review if x not in string.punctuation]\n",
    "        tokens = [x for x in tokens if x.isalnum()]\n",
    "        with open('stopwords.txt') as f:\n",
    "            stopwords = f.read().splitlines()\n",
    "        tokens = [x for x in tokens if x not in stopwords]\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "train['Tokens'] = train['Review'].apply(lambda a: tokenize(a))\n",
    "test['Tokens'] = test['Review'].apply(lambda a: tokenize(a))\n",
    "train.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Word2Vec [Loading word Vector - 2 points, Feature matrix - 8 points]\n",
    "\n",
    "Use the glove.6B.50d.txt to load the 50 dimensional word vector. In the RNN tutorial, you were taught how to load a pre-computed embedding. Load this file the same way. \n",
    "The above gives you word embeddings for the words. A review is composed of words. Given a review, the review embedding is the average of the individual token(word) embeddings. Write a function to create a matrix whose rows are the review embeddings created as described in the previous sentence. If a word does not have an embedding in the above glove model, ignore it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_rev = np.array(train.Tokens)\n",
    "train_label = np.array(train.Sentiment)\n",
    "test_rev = np.array(test.Tokens)\n",
    "test_pred = np.array(test.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "embeddings_index = {}\n",
    "with open(\"glove.6B.50d.txt\") as f:\n",
    "    # Each row represents a word vector\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # The first part is word\n",
    "        word = values[0]\n",
    "        # The rest are the embedding vector\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "def createFeatureMatrix(review):\n",
    "    embedding_matrix = np.zeros((len(review), embedding_dim))\n",
    "    i = 0 \n",
    "    cnt = 0\n",
    "    for rev in review:\n",
    "        vec = np.zeros(embedding_dim)\n",
    "        c=0\n",
    "        for token in rev:\n",
    "            c+=1\n",
    "            if token in embeddings_index:\n",
    "                vec+=embeddings_index[token]\n",
    "            else:\n",
    "                cnt+=1\n",
    "        vec = vec/c\n",
    "        embedding_matrix[i] = vec\n",
    "        i+=1\n",
    "    print(cnt)\n",
    "    return embedding_matrix\n",
    "        \n",
    "train_mat = createFeatureMatrix(train_rev)\n",
    "test_mat = createFeatureMatrix(test_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) SVM [Cross Validation - 6 points, Correct Prediction - 4 Points]\n",
    "Pass the above created training matrix through Polynomial Kernel SVM.\n",
    "Do a grid search in SVM over the best c in the range np.logspace(-4, 2, 3)  and best degree in the range range range(1,4)\n",
    "Predict the rating on SVM and report the accuracy. <br>\n",
    "NOTE: You have to implement your own grid search cross validation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51751256281407032, 0.51751256281407032, 0.51751256281407032]\n",
      "[0.51751256281407032, 0.51751256281407032, 0.51751256281407032]\n",
      "[0.73772864321608034, 0.73072864321608033, 0.70870854271356776]\n",
      "[0.51751256281407032, 0.51751256281407032, 0.73772864321608034]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_folds = 5\n",
    "k_fold = KFold(n_folds)\n",
    "Cs = np.logspace(-4, 2, 3)\n",
    "Ds = range(1,4)\n",
    "\n",
    "c_scores = []\n",
    "bestD = []\n",
    "for C in Cs:\n",
    "    d_scores= []\n",
    "    for d in Ds:\n",
    "        fold_scores = []\n",
    "        for k, (train, val) in enumerate(k_fold.split(train_mat, train_label)):\n",
    "            clf = svm.SVC(C=C,kernel=\"poly\",degree=d)\n",
    "            clf.fit(train_mat[train], train_label[train])\n",
    "\n",
    "            ypred = clf.predict(train_mat[val])\n",
    "            yval = train_label[val]\n",
    "            accuracy = np.sum(ypred==yval)/float(len(ypred))\n",
    "            fold_scores.append(accuracy)\n",
    "        d_score = np.mean(fold_scores)\n",
    "        d_scores.append(d_score)\n",
    "    print(d_scores)\n",
    "    ind = np.argmax(d_scores)\n",
    "    bestD.append(Ds[ind])\n",
    "    c_scores.append(max(d_scores))\n",
    "print(c_scores)\n",
    "ind = np.argmax(c_scores)\n",
    "bestC= Cs[ind]\n",
    "bestD = bestD[ind]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(bestC)\n",
    "print(bestD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=1, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf = svm.SVC(C=bestC,degree=bestD,kernel=\"linear\")\n",
    "clf.fit(train_mat, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73092369477911645"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(test_mat)\n",
    "accuracy_score(test_pred, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
