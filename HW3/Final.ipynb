{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865 Fall 2018 Final Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your name: Ruixin Huang\n",
    "\n",
    "Your Andrew ID: ruixinh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preamble**: You will need to install `joblib`, which can be done via the terminal command below:\n",
    "\n",
    "```\n",
    "conda install -c anaconda joblib\n",
    "```\n",
    "\n",
    "When loading `fashion_mystery.hd5` in Problem 2(c), if you run into an error saying `ValueError: Unknown layer:name`, please update your version of `keras` so that the file actually loads. A related post: https://stackoverflow.com/questions/53180589/keras-valueerror-unknown-layername-when-trying-to-load-model-to-another-platf?noredirect=1&lq=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three problems on this exam. You can do these three problems in any order. Problem 1 is a short warmup problem *with no coding*. Problems 2 and 3 involve coding. We have prepared a bunch of imports for you in the next cell. Please execute this before you do problems 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import seaborn as sn\n",
    "plt.style.use('ggplot')\n",
    "# %config InlineBackend.figure_format = 'retina'  # if you use a Mac with Retina display\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import joblib\n",
    "\n",
    "# these next three lines are meant to turn off GPU usage\n",
    "# (for this final exam, you do not need a GPU)\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Warm-Up [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Answer if the given statement is true or false, and provide your reason: \"Stochastic gradient descent is a special case of minibatch gradient descent.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: The statement is ture, I think the reason is In the stochastic gradient descent method, the parameters in the model are adjusted according to only one sample at a time, which is equivalent to the mini-batch gradient descent in the case of b=1 described above, that is, there is only one training sample in each mini-batch. So, this could be explained why \"Stochastic gradient descent is a special case of minibatch gradient descent.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Consider a length-3 sequence [x, y, z] (if you want, you can think of this as an image with 1 row and 3 columns, where the pixel's have values x, y, and z]). We feed this sequence as the input to a convolutional layer with two filters [a, b] and [c, d]. This convolutional layer does not do any padding. What is the output of this convolutional layer in terms of the variables x, y, z, a, b, c, d?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: the output concolutional layer one after using filter [a,b] is [x*a+x*b,y*a+y*b,z*a+z*b], the output convolutional layer two after using filter[c,d] is [x*c+x*d,y*c+y*d,z*c+z*d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Your manager wants you to build an algorithm to detect if news articles being featured on your website are fake or not. You train a probabilistic classifier such as logistic regression which treats fake articles as the positive class and real news articles as the negative class. You show your manager the accuracy, precision, and recall of your trained model. Your manager states that it would be great if the algorithm could be tuned so that the anti-abuse team at your company only has to investigate the news articles which are marked with the highest probability of being fake while not generating too many false positives which the team has to investigate needlessly. In this situation, will you use the precision-recall curve or the receiver operating characteristic (ROC) curve to decide the probability threshold above which news articles are marked as fake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**:  Yes, I would use PRC or ROC model, due to setting the real new articals as the negative class. In the real situation, the quantity of real articals always much more than fake articals, we hope we can check all of the fake news, even sacrify the accurancy. Besides, we also will hope we can check each new in the right situation, we don't want to serve real news become fake news. Because of this, we need to make sure the recall is low. Therefore, it is useful to use PRC and ROC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Answer if the given statement is true or false, and provide your reason: \"The cross-entropy loss is applicable only to binary classification. For number of classes beyond 2, we need to use more general losses such as KL divergence.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: This is false. because in term of any model trained by the gradient optimization method, minimizing the cross-entropy between the data distribution and the model distribution gives the same result as the minimized KL divergence. cross-entropy not only suit for binary, but also suit for mutiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Answer if the given statement is true or false, and provide your reason: \"t-SNE dimensionality reduction algorithm models the distribution of distances in both the original and embedding space using Student's t-distribution.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: It is false. Because Studen's t-distribution in t-SNE is used to figure out the similarity in embedding space instead of distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Dimensionality reduction and prediction [50 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main concepts in 95-865 is that even though unstructured data often come from high-dimensional feature spaces, they actually usually exhibit some sort of low-dimensional structure. In the first half of the class, we looked at *unsupervised* dimensionality reduction. In this problem, we look at how combining unsupervised dimensionality reduction method (e.g., PCA, t-SNE) with a classifier compares with an \"end-to-end\" neural net approach that tries to learn everything at once. To answer this question, we focus on an image classification and examine four different approaches:\n",
    "\n",
    "(a) No dimensionality reduction at all: use the raw features for classification.\n",
    "\n",
    "(b) Reduce the dimensionality of the data using PCA, and *use the low-dimensional PCA representation* of the data for classification.\n",
    "\n",
    "(c) The same thing as approach (b) except where we use Isomap instead of PCA.\n",
    "\n",
    "(d) A simple neural net approach that automatically learns a low-dimensional representation while also making predictions.\n",
    "\n",
    "For approaches (a), (b), and (c), we use logistic regression (generalized to multiple classes) as the classifier (so that the classifier used in all four approaches corresponds to logistic regression). Note that for approaches (a), (b), and (c), your code should not use `keras`.\n",
    "\n",
    "**Data**: In exploring these approaches, we use the Fashion MNIST dataset, which you saw during the mid-mini quiz. As a reminder, this dataset is collected by Zalando Research and is similar to the popular MNIST dataset for handwritten digit recognition that we have seen throughout 95-865. Each image is 28-by-28 pixels and has a numerical label from 0, 1, ..., up to 9 (**0 → T-shirt/top, 1 → Trouser, 2 → Pullover, 3 → Dress, 4 → Coat, 5 →\n",
    "Sandal, 6 → Shirt, 7 → Sneaker, 8 → Bag, 9 → Ankle boot**). Example images from the dataset are shown below.\n",
    "\n",
    "![Example Images](fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already curated training and test data for you. Please run the next cell before moving on to the subsequent parts. Note that each image has been flattened already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 784) (9000,)\n",
      "(1000, 784) (1000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.loadtxt('fashion_X_train.txt')\n",
    "y_train = np.loadtxt('fashion_y_train.txt')\n",
    "X_test = np.loadtxt('fashion_X_test.txt')\n",
    "y_test = np.loadtxt('fashion_y_test.txt')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2(a): Multiclass logistic regression without dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the training data, train a classifier using scikit-learn's `LogisticRegression` class (from `sklearn.linear_model`). Please use the following parameters when creating your `LogisticRegression` object (do not set any of the other parameters):\n",
    "\n",
    "- Set `multi_class` to `'multinomial'` (so that the classifer uses categorical cross entropy loss)\n",
    "- Set `solver` to `'lbfgs'`\n",
    "- Set `random_state` to 0\n",
    "- Set `C` to 10000 (intentionally reduce the amount of regularization)\n",
    "\n",
    "After training the classifier, compute and print the prediction accuracy (fraction of correct predictions) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8967777777777778"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "clf = LogisticRegression(random_state=0,solver='lbfgs',multi_class='multinomial',C=10000)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_train,y_train)\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2(b): PCA with multiclass logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2(b)-Subpart (i)\n",
    "\n",
    "Now we are going to try the following approach:\n",
    "\n",
    "1. Reduce the dimensionality of the input data using PCA\n",
    "2. Train a logistic regression classifier using the transformed (i.e., low-dimensional) representation of the input data as feature vectors (and using the original labels); for the logistic regression classifier, use the exact same algorithm parameters as for part (a), including `random_state=0`\n",
    "\n",
    "Note that there is a single hyperparameter here: the number of dimensions to reduce to. Use 5-fold cross validation (using raw accuracy) to select this number of dimensions from integers 10, 20, 30, ..., 100. Importantly, at no point should your training procedure for doing the above two steps ever be looking at test data (`X_test` and `y_test`). **After doing the cross validation, plot the cross validation score vs PCA dimensionality and also be sure to store and print out what the best number of dimensions is.**\n",
    "\n",
    "To be clear: how one trains a model for this proposed approach is to first fit a PCA model, use it to obtain the lower-dimensional representation, and then use the low dimensional representation to train a logistic regression classifier. When using the model to make predictions, you should *not* be doing any refitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7200, 9000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a1b5ed41fe28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_std\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_std\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0myval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m-> 1216\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7200, 9000]"
     ]
    }
   ],
   "source": [
    "# WARNING: YOU SHOULD ONLY HAVE TO WRITE CODE IN THE PART BELOW THAT SAYS \"TODO\"\n",
    "kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "best_cross_val_score = -np.inf\n",
    "best_PCA_dim = None\n",
    "low_dim_range = range(10, 101, 10)\n",
    "cross_val_scores = []\n",
    "\n",
    "for low_dim in low_dim_range:\n",
    "    # --------------------------------------------------------------------------\n",
    "    # TODO: WRITE YOUR CODE HERE\n",
    "    #\n",
    "    dimension_pca = PCA(n_components=low_dim)\n",
    "    X_train_std = dimension_pca.fit_transform(X_train)\n",
    "    ytrain_std = y_train.T\n",
    "    cross_val_score = -1  # this value is obviously wrong and is for you to fill\n",
    "                          # in with the correct 5-fold cross-validation score;\n",
    "                          # remember to use the same algorithm parameters for\n",
    "                          # logistic regression as in part (a), including\n",
    "                          # setting `random_state=0`  \n",
    "    for k, (train, val) in enumerate(kf.split(X_train, y_train)):\n",
    "        clf = LogisticRegression(random_state=0)\n",
    "        clf.fit(X_train_std[train], y_train)\n",
    "        ypred = clf.predict(X_train_std[val])\n",
    "        yval = y_train[val]\n",
    "        cross_val_scor = clf.score(ypred,yval)\n",
    "        \n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    cross_val_scores.append(cross_val_score)\n",
    "    print('# PCA dim:', low_dim, 'cross val score:', cross_val_score)\n",
    "    if cross_val_score > best_cross_val_score:\n",
    "        best_cross_val_score = cross_val_score\n",
    "        best_PCA_dim = low_dim\n",
    "\n",
    "plt.plot(low_dim_range, cross_val_scores)\n",
    "plt.xlabel('PCA dimensionality')\n",
    "plt.ylabel('Cross validation score (raw accuracy)')\n",
    "\n",
    "print('Best #PCA dim:', best_PCA_dim,\n",
    "      'best cross val score:', best_cross_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2(b)-Subpart (ii)\n",
    "\n",
    "Using the best number of PCA dimensions found, train the model using all training data. What is the prediction accuracy (fraction of correct predictions) on the test data (print this accuracy)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2(c): Isomap with multiclass logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the analysis for problem 2(b) except using Isomap instead of PCA. Because the analysis using Isomap is extremely slow, we run most of the analysis for you ahead of time. In fact, here's the 5-fold cross-validation plot varying the Isomap dimensionality (sweeping over the same values as in problem 2(b)):\n",
    "\n",
    "![Isomap cross validation](fashion_isomap_cross_val.png)\n",
    "\n",
    "The best Isomap dimensionality found is 100, which achieves a cross validation score of 0.791.\n",
    "\n",
    "Subpart (i) below asks how you would produce the above plot with code (we aren't asking you to actually compute it since it would take too long to run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2(c)-Subpart (i)\n",
    "\n",
    "Clearly and concisely state how you would change your answer to problem 2(b) to use scikit-learn's [Isomap](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap) class instead of PCA (for Isomap, assume that you use all the default parameters except for `n_components`, which refers to the number of dimensions to reduce to). Your answer should include a code snippet in it, but the code snippet is *not* meant to be run (since it would be very slow to run during the exam!).\n",
    "\n",
    "**Note**: While we did not use scikit-learn's Isomap class in any of the lectures/recitations, the Isomap class is used in an *extremely* similar way to scikit-learn's PCA class. To answer this problem, you do not actually need to use any knowledge of what happens under the hood for Isomap (for example, your answer should not include any code for multidimensional scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (there should be a code snippet in it)**: Because use PCA would decrease the accuracy in reduce dimensions.\n",
    "\n",
    "```\n",
    "YOUR CODE SNIPPET GOES HERE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2(c)-Subpart (ii)\n",
    "\n",
    "Using the full training dataset, we trained Isomap (using 100 dimensions) followed by multiclass logistic regression. The pre-trained Isomap model was ~1.5GB, so instead of giving you that, we have gone ahead and transformed the test feature vectors to 100-dimensional vectors using our pre-trained Isomap model. Using these lower-dimensional representations of the test data, along with the pre-trained multiclass logistic regression model, compute and print out the prediction accuracy (fraction of correct predictions) for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THESE FIRST TWO LINES\n",
    "X_test_isomap = np.loadtxt('fashion_X_test_isomap.txt')\n",
    "log_reg_after_isomap = joblib.load('fashion_mystery.joblib')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2(c)-Subpart (iii)\n",
    "\n",
    "Why wouldn't your answer to subpart (i) work if we had used t-SNE? Very briefly explain.\n",
    "\n",
    "**Hint**: From a quick look at scikit-learn's documentation for both Isomap and t-SNE, you should be able to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2(d): a neural net approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run 5-fold cross-validation already using a CNN, where we only varied the size of a Dense layer right before the final multiclass logistic regression layer/classifier. We set the number of neurons to be 10, 20, ..., up to 100, just like with problems 2(b) and 2(c). We got the following plot:\n",
    "\n",
    "![Fashion MNIST CNN 5-fold cross-validation](fashion_cnn_cross_val.png)\n",
    "\n",
    "The best choice for the dimensionality of the space prior to the final logistic regression layer turned out to be 40. We then trained the CNN on all the training data using dimensionality 40 for the dense layer before the logistic regression. We saved the resulting model in `fashion_mystery.hd5`. We first load the model and print out its summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_125 (Conv2D)          (None, 24, 24, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_125 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 10, 10, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_126 (MaxPoolin (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_63 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 40)                32040     \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 10)                410       \n",
      "=================================================================\n",
      "Total params: 52,578\n",
      "Trainable params: 52,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = load_model('fashion_mystery.hd5')\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2(d)-Subpart (i)\n",
    "\n",
    "What are the sizes of the convolution filters used in the Conv2D layers? (Also, briefly explain how you determined this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (no code)**: I think we need to use small size of filter such as 3*3 or 5*5, because out dataset is not big, it can improve the efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2(d)-Subpart (ii)\n",
    "\n",
    "What is the prediction accuracy (fraction of correct predictions) on the test set using the CNN?\n",
    "\n",
    "*Hint*: To answer this question, you will need to remember to transform your test feature vectors and labels so that they are shaped the right way for the neural net to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2(d)-Subpart (iii)\n",
    "\n",
    "Compute the confusion matrix for the test data using the CNN. To do this, use scikit-learn's [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) function. Note that the rows index the true labels, and the columns index the predicted labels. Also compute a normalized version of this table where each row is divided by the row sum in the original confusion matrix. We have already provided plotting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "conf_mat = np.zeros((10, 10))  # fill this out correctly\n",
    "normalized_conf_mat = np.zeros((10, 10))  # fill this out correctly\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sn.heatmap(conf_mat, annot=True,\n",
    "           xticklabels=classes,\n",
    "           yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sn.heatmap(normalized_conf_mat, annot=True,\n",
    "           xticklabels=classes,\n",
    "           yticklabels=classes)\n",
    "plt.title('Normalized Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the normalized confusion matrix, what do the different numbers represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: predict is in the column, actual is in the line, all the right prediction is on the diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about two largest off-diagonal entries (please relate these back to what the labels represent, e.g., trousers, dresses, etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: In general ,they are right prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What item type do you think is easiest (most accurate) for the CNN to classify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: improve the Convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2(e): Observations about approaches (a) through (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four approaches we just examined in this problem involve using a multiclass logistic regression classifier. However, we changed the input given to this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the number of dimensions (in the low dimensional space) chosen for PCA and Isomap compared to that of the CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the test set prediction accuracy of the PCA-based and Isomap-based approaches compared to the multiclass logistic regression classifier of part (a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think the test set accuracies turned out the way they did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Predicting Diabetes Cases [40 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The National Institute of Diabetes and Digestive and Kidney Diseases has collected the Pima Indians dataset, which contains data from clinical exams for women age 21 and above of Pima Indian origins. In this problem, we predict whether each woman has diabetes or not based on diagnostic measurements.\n",
    "\n",
    "For each woman, the following features were recorded:\n",
    "- Pregnancies: Number of times pregnant\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "- Blood Pressure: Diastolic blood pressure (mm Hg)\n",
    "- Skin Thickness: Triceps skin fold thickness (mm)\n",
    "- Insulin: 2-Hour serum insulin (mu U/ml)\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "- Diabetes Pedigree Function: Diabetes pedigree function\n",
    "- Age: Age (years)\n",
    "\n",
    "Per woman, the label is binary (1=diabetic, 0=not diabetic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 8) (576,)\n",
      "(192, 8) (192,)\n"
     ]
    }
   ],
   "source": [
    "diabetes_X_train = np.loadtxt('diabetes_X_train.txt')\n",
    "diabetes_y_train = np.loadtxt('diabetes_y_train.txt')\n",
    "diabetes_X_test = np.loadtxt('diabetes_X_test.txt')\n",
    "diabetes_y_test = np.loadtxt('diabetes_y_test.txt')\n",
    "diabetes_features = ['Pregnancies',\n",
    "                     'Glucose',\n",
    "                     'Blood Pressure',\n",
    "                     'Skin Thickness',\n",
    "                     'Insulin',\n",
    "                     'BMI',\n",
    "                     'Diabetes Pedigree Function',\n",
    "                     'Age']\n",
    "print(diabetes_X_train.shape, diabetes_y_train.shape)\n",
    "print(diabetes_X_test.shape, diabetes_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3(a)\n",
    "\n",
    "Explore the training data by drawing a histogram for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XlcVPX6wPEPMApmpiKZ4VLXtdKuloZmpgiDIi5o4tfKq6amLVoudS1Mu71KTStTvHk1SyvLm35Tc8s0lzTTW7n8cum2iErhEi64YpAD8/tjDnNZhYFhZuA879eLF2f5nnOeOQzPnPmec57jZ7fbEUIIYR7+3g5ACCGEZ0niF0IIk5HEL4QQJiOJXwghTEYSvxBCmIwkfiGEMBlJ/EIIYTKS+IUQwmQk8QshhMlYvB2AQW4fFkKIkvFzdQFfSfycOHHCpfYhISGcOXOmjKIpO+UxbonZc8pj3BKz5+SNOzQ0tETrka4eIYQwGUn8QghhMpL4hRDCZCTxCyGEyUjiF0IIk5HEL4QQJiOJXwghTEYSvxBCmIwkfiGEMBlTJv7Q7w8Q+v0Bb4chhBBeYcrEL4QQZiaJXwghTEYSvxBCmIwkfiGEMBlJ/EIIYTKS+IUQwmQk8QshhMlI4hdCCJORxC+EECYjiV8IIUxGEr8QQpiMJH4hhDAZSfxCCGEyluI0UkolAZeATMCmtW6jlAoGlgK3AkmA0lqfU0r5AQlADHAFeERrvdf9oQshhCgJV474O2utW2mt2xjjzwObtdZNgM3GOEA3oInxMwKY665ghRBClF5punpigQ+M4Q+A3jmmL9Ja27XW3wA1lFI3l2I7Qggh3Ki4id8OfKGU2qOUGmFMu0lrfdIY/h24yRiuCyTnWPaYMU0IIYQPKFYfP9BBa31cKVUb2KiU+innTK21XSlld2XDxgfICGN5QkJCXFkci8Xi8jJ5lXb5knBH3J4mMXtOeYxbYvYcd8VdrMSvtT5u/D6llPoUCANSlFI3a61PGl05p4zmx4H6ORavZ0zLu875wHxj1H7mzBmXAg8JCcHVZbKFGr9LunxplCZub5GYPac8xi0xe07euENDQ6/RunBFdvUopaoqpaplDwNdgIPAamCw0WwwsMoYXg0MUkr5KaXaARdydAkJIYTwsuL08d8EfK2U2gd8B3ymtV4PTAOilFKHAKsxDrAOOAIkAu8AT7o9aiGEECVWZFeP1voI0LKA6WeByAKm24GRbolOCCGE28mdu0IIYTKS+IUQwmQk8QshhMlI4hdCCJORxC+EECYjiV8IIUxGEr8QQpiMJH4hhDAZSfxCCGEykviFEMJkJPELIYTJSOIXQgiTkcQvhBAmI4lfCCFMRhK/EEKYjCR+IYQwGUn8QghhMpL4hRDCZCTxCyGEyUjiF0IIk5HEL4QQJiOJXwghTEYSvxBCmIwkfiGEMBlJ/EIIYTKS+IUQwmQk8QshhMlI4hdCCJORxC+EECZjKW5DpVQAsBs4rrXuoZT6C7AEqAXsAQZqrf9USgUCi4DWwFmgv9Y6ye2RCyGEKBFXjvhHAz/mGJ8OzNRaNwbOAcOM6cOAc8b0mUY7IYQQPqJYiV8pVQ/oDrxrjPsBEcAyo8kHQG9jONYYx5gfabQXQgjhA4rb1TMLGA9UM8ZrAee11jZj/BhQ1xiuCyQDaK1tSqkLRvszOVeolBoBjDDaERIS4lrgFovLy+RV2uVLwh1xe5rE7DnlMW6J2XPcFXeRiV8p1QM4pbXeo5QKL/UWDVrr+cB8Y9R+5syZazXPJyQkBFeXyRZq/C7p8qVRmri9RWL2nPIYt8TsOXnjDg0NvUbrwhWnq+c+oJdSKgnHydwIIAGooZTK/uCoBxw3ho8D9QGM+dVxnOQVQgjhA4pM/FrreK11Pa31rcCDwBat9QDgSyDOaDYYWGUMrzbGMeZv0Vrb3Rq1EEKIEivNdfzPAeOUUok4+vAXGNMXALWM6eOA50sXohBCCHcq9nX8AFrrrcBWY/gIEFZAm3SgnxtiE0IIUQbkzl0hhDAZSfxCCGEykviFEMJkJPELIYTJSOIXQgiTkcQvhBAmI4lfCCFMRhK/EEKYjCR+IYQwGUn8QghhMpL4hRDCZCTxCyGEyUjiF0IIk5HEL4QQJiOJXwghTEYSvxBCmIwkfiGEMBlJ/EIIYTIuPXpRiIrAbreTnp5OVlYWfn5+3g4nl5SUFDIyMrwdhksk5rJlt9vx9/cnKCjIbeuUxC9MJz09nUqVKmGx+N7b32KxEBAQ4O0wXCIxlz2bzUZ6errb1iddPcJ0srKyfDLpC1EYi8VCVlaW29YniV+Yjq917whRHO5830riF8IL6tevT1RUFBEREYwYMYI//vjD2yEVW69evbwdgigl+b4rTC9zuHsTWcA7q4tsExQUxMaNGwEYNWoUixYt4rHHHnPOt9vtzpN6vmb16qJfn/BtvveuEsJkwsLCSEpKIjk5mfbt2/P0008TERHBiRMn2LZtGz179qRr166MGDGCtLQ0ADZv3kzHjh2Jjo5m0qRJDBo0CIAZM2Ywbtw44uLiuPfee1mwYIFzO0OHDiU6OprOnTvz0UcfOac3adKEadOmYbVa6dGjB6dPnwbg9OnTDBs2DKvVitVqZdeuXc722ebOnUvXrl2xWq288cYbAFy5coWBAwditVqJiIhg1apVZbsDhcsk8QvhRTabjS+//JLbbrsNgCNHjjB48GC+/PJLrrvuOhISEli6dCkbNmygZcuWzJ8/n/T0dJ577jk++ugj1q9fz9mzZ3OtMzExkcWLF/PZZ5/x5ptvcvXqVcDxobB+/XrWrVvHwoULSU1NBRyJ+u6772bTpk20a9eOxYsXAzBp0iTatWvHpk2b2LBhA82aNcu1nW3btnH06FHWr1/PF198wf79+/nmm2/48ssvqVOnDps2bWLLli107ty5rHejcJF09QjhBenp6URFRQHQtm1bHnroIVJSUqhXrx6tW7cGYM+ePfzyyy/ExsYCcPXqVVq3bk1iYiK33HILDRo0AKB37965juAjIyMJDAwkMDCQkJAQTp8+TWhoKAsXLuTzzz8H4MSJExw9epTg4GAqV67sjOXOO+9k+/btAOzYsYOEhAQAAgICuOGGG3K9hm3btrFt2zYiIyOx2+1cuXKFo0ePEhYWxssvv8yUKVOwWq20bdu2rHajKCFJ/EJ4Qc4+/pyuu+4657Ddbqdjx47861//ytXm4MGD11x3YGCgczggIIDMzEx27tzJ9u3bWbNmDVWqVCEuLs55A5PFYnFeMRIQEIDNZivWa7Db7YwaNYohQ4bkW2b9+vVs2bKF1157jQ4dOjB27NhirVN4hnT1COGjWrduza5duzh69Cjg6JI5fPgwjRo14tdffyU5ORko3snWS5cuUb16dapUqUJiYiJ79+4tcpkOHTqwaNEiADIzM7l48WKu+eHh4SxdutR53uHkyZOcOXOG33//nSpVqtC3b18ef/xxDhw44NLrFmWvyCN+pVQQ8BUQaLRfprX+h1LqL8ASoBawBxiotf5TKRUILAJaA2eB/lrrpDKKX4gKq1atWsycOZORI0fy559/AjB+/HgaNWrE1KlTGTBgANdddx0tW7Yscl3h4eF8+OGHdOrUiUaNGnH33XcXuczLL7/M+PHjWbJkCf7+/rz66qu0adPGOb9Tp04cOnSImJgYwPFt5Z///CdJSUlMnjwZPz8/KlWqxKuvvlrCPSDKip/dbr9mA6WUH1BVa31ZKVUJ+BoYDYwDVmitlyil5gH7tNZzlVJPAn/VWj+ulHoQ6KO17l9EHPYTJ064FHhISAhnzpxxaZlsod87jkBOtLqzRMuXRmni9paKFvOVK1dydan4EovFUqyulrS0NKpWrYrdbmfChAn85S9/YcSIER6IML/ixuxLymPMV65coUGDBrne16GhoQAu39lVZFeP1tqutb5sjFYyfuxABLDMmP4B0NsYjjXGMeZHGh8eQgg3Wbx4MVFRUXTu3JlLly4xcOBAb4ckypFindxVSgXg6M5pDMwBDgPntdbZH5nHgLrGcF0gGUBrbVNKXcDRHXQmzzpHACOMdoSEhLgWuMXi8jJ5lXb5knBH3J5W0WJOSUnx6Vo9xYntySef5Mknn/RANMXjy/uzMOUt5sDAQLf9LxbrlWutM4FWSqkawKfAbaXdsNZ6PjDfGLW72pVQqq4e47c3ui8qWreJr7pWzBkZGT5bmbE8dkFIzJ6RkZGBzWYrqKvHZS5d1aO1Pg98CdwL1FBKZX9w1AOOG8PHgfoAxvzqOE7yCiGE8AFFJn6l1I3GkT5KqSpAFPAjjg+AOKPZYCD7vuzVxjjG/C1a62ufQRZCCOExxTnivxn4Uim1H9gFbNRarwWeA8YppRJx9OFnFwVZANQypo8Dnnd/2EIIIUqqyD5+rfV+4K4Cph8BwgqYng70c0t0QlRgp0+f5qWXXmLv3r1Ur16dSpUq8dRTT3H99dczb948581TQrhb+TqtLUQZiF38k1vXt2pA0dc+2O12hg4dSr9+/ZgzZw4Ax44dY9OmTTRt2tSt8QiRl5RsEMILvv76aypXruwspwxQr149Hn300VztZsyYwbx585zjERERzlINn3zyibNk8lNPPQVAcnIy/fr1w2q1opTi+HHHNRdr1qwhIiICq9XKAw88ADjKMLzyyivExMRgtVr58MMPy/Q1C99hiiP+2onxAJxqLLeOC9/wyy+/0KJFixIv//PPP5OQkMDq1asJDg7m3LlzAEycOJF+/fqhlGLJkiVMmjSJhQsXMmvWLBYvXszNN9/MhQsXAPj444+pVq0a69atIyMjg969e9OpUydn1U9RcckRvxA+YMKECVitVrp27Vqs9jt27KBHjx4EBwcDULNmTcBRyrlPnz4A9O3bl++++w6ANm3aMHbsWBYvXkxmZibgKKu8bNkyoqKi6NGjB+fOnXMWhBMVmymO+IXwNU2bNmXdunXO8alTp5KamuoseJYtICCArKws53h2KWVXTZ8+nb1797J582a6devmrMs/efJkwsPDS7ROUX7JEb8QXtChQwcyMjL44IMPnNMKeuB6/fr1nWWNDxw4wG+//QbAfffdx9q1a51P0cru6mnTpo3zUYcrVqxwPgQlKSmJu+++m7///e/UqlWLEydO0KlTJxYtWuR8Qtfhw4e5cuVKGb1i4UvkiF8IL/Dz82PBggW89NJLzJ07l1q1alGlShUmTpyYq11MTAzLli2jc+fO3HXXXTRs2BCAZs2a8fTTTxMXF4e/vz8tWrRg1qxZTJ48mbFjxzJv3jyCg4OZOXMm4DiyP3r0KHa7nQ4dOtC8eXPuuOMOkpOTiY6Oxm63ExwczMKFCz2+L4TnFVmW2UPKtCxz3pO7UpbZNRUt5opQltmXSMye4dGyzEIIISoWSfxCCGEykviFEMJkJPELIYTJSOIXQgiTkcQvhBAmI4lfCC+oX78+UVFRzjINu3btAuC3334jIiLCLduIi4tj3759BU6///77sVqtxMbGkpiY6JbtifJDbuASprdm6Xm3rq9n/xpFtgkKCmLjxo0AbN26lWnTprF8+XK3xnEtb731Fi1btuSjjz5i8uTJvP/++7nmZ2Zmeuy5xDabrdw9+Ly8kyN+Ibzs0qVLVK9ePd/09PR0xo4dS2RkJF26dGHHjh3XnP7HH3/wxBNP0KlTJ4YNG0Z6enqR227Xrh1JSUkAtG3blilTptC1a1fWrl1LUlISAwYMIDo6mj59+ji/GRRU4vnnn3+me/fuzm8xR44cITk5Ode3l3nz5jFjxgzA8a3jxRdfpFu3brz77rucPXuW4cOHExMTQ0xMjPMbkCgb8jErhBekp6cTFRVFRkYGp06dQmudr83777+Pn58fmzdvJjExkYceeojt27cXOn3RokVUqVKFbdu28d///pfo6Ogi49i4cSO33fa/B8fUrFmTDRs2AKCUYtq0aTRs2JC9e/cSHx/PJ598UmCJ5w8//JBhw4bxwAMP8Oeff5KZmVnk3d5Xr151FosbOXIkw4cPJywsjOPHj/Pwww+zbdu2Yu9P4RpJ/EJ4Qc6unt27dzN69Gi2bNmSq82uXbsYMmQIAI0bN6ZevXocOXKk0OnffvstQ4cOBeCOO+7g9ttvL3T7o0aNIigoiPr16/PKK684p/fq1QuAtLQ09uzZw2OPPeac9+effwL/K/Hcs2dPunXrBkDr1q2ZPXs2J0+epFu3bs6aQteSvS2A7du388svvzjHL1++TFpaGlWrVi1yPcJ1kviF8LI2bdqQmprK2bNnPbbN7D7+vLJrGGVlZXHDDTc4P5xyylvieePGjfTp04e77rqLzZs3M3DgQKZPn07Dhg1zlZTO2/WUs15SVlYWa9asISgoyF0vUVyD9PEL4WWJiYlkZmY6H6aSLSwsjE8//RRwlEw+fvw4jRo1KnR627ZtWblyJQA//fQTP/74Y4ljqlatGvXr12fNmjWA4xnBP/zwA1Bwiedff/2VW265hWHDhtG1a1d+/PFHbrzxRs6cOUNqaioZGRls2rSp0O116tSJ9957zzl+8ODBEscuiiZH/EJ4QXYfPziS6qxZs/JdRTN48GDi4+OJjIwkICCAmTNnEhgYWOj0QYMGMW7cODp16kSTJk3461//WqoY33rrLeLj40lISMBmsxEbG0vz5s0LLPGckJDA8uXLsVgs1K5dm6eeeopKlSoxduxYevToQZ06dWjcuHGh23rllVecTyGz2Wy0bduW6dOnlyp+UTgpy+xhFa3Esa+SssyeIzF7hpRlFkIIUWKS+IUQwmQk8QshhMlI4hdCCJORxC+EECZT5OWcSqn6wCLgJsAOzNdaJyilgoGlwK1AEqC01ueUUn5AAhADXAEe0VrvLZvwhRBCuKo4R/w24Bmt9R1AO2CkUuoO4Hlgs9a6CbDZGAfoBjQxfkYAc90etRDlXEJCAp07d8ZqtRIVFcXevY5jo+y7ePPKWd6gKBMmTCAqKorw8HAaNWpEVFQUUVFRrF27ttBSzfv27WPSpEnXXG+TJk2KHYPwbUUe8WutTwInjeFLSqkfgbpALBBuNPsA2Ao8Z0xfpLW2A98opWoopW421iOEz5k9e7Zb1/f0009fc/7u3bvZtGkT69evJzAwkNTUVGcdnMKsXr262NufOnUqAMnJyQwePDhX2YW85ZeztWzZssASDqJicqmPXyl1K3AX8C1wU45k/juOriBwfCgk51jsmDFNCAGcOnWK4OBgAgMDAQgODqZOnTq52vzxxx/87W9/Y/HixcD/jrZ37txJXFwcw4cPp2PHjowaNQpXb8Jcu3Yt3bt3p0OHDnz77bfO9Q4aNAhwFGjLLvtstVr57LPPci2fmppKz5492bRpEzt37qRPnz4FxrN//3769u1LdHQ0Dz/8MCkpKQAsWLCA8PBwrFYrTzzxBAD/+c9/nN9MunTpwuXLl116TcI1xS7ZoJS6HlgOjNFaX1RKOedpre1KKZfefUqpETi6gtBaExIS4sriWCyW4i9jPGAob3tXt+kOLsXtIypazCkpKWX64I+i1h0ZGcmsWbO4//776dixI7GxsbRv3945Pz09nSeffBKlFDn/zywWCwEBARw8eJCvvvqKOnXq0KNHD/bu3Uvbtm3zbSe7BETOePz8/MjKymLDhg1s2rSJmTNnsmzZMgICAvDz88NisTB79myqV6/uLIt8/vx55zpSU1MZNGgQ8fHxdOrUiR07dnDgwIF88dx9991MmjSJDz74gJCQEFauXMlrr71GQkICc+bMYdeuXQQGBnLhwgUsFgtvv/0206dPJywsjLS0NAIDA8v84Szl7eEv2fvEHf+LxXrlSqlKOJL+Yq31CmNySnYXjlLqZuCUMf04UD/H4vWMablorecD841Ru6slAVwq2WD8zm4fmmfckypa+QNfda2YMzIyyvTpUkWVAggMDOTzzz/n22+/ZefOnYwYMYL4+Hj69+8PwKBBg3jyySd54IEHcq3LZrORmZlJq1atqF27NllZWdxxxx0kJSXRunXrfNvJzMzMF4/dbic6OhqbzUbz5s1JTk52rtdut2Oz2di2bRv/+te/nMtdf/312Gw2bDYbcXFxTJkyhXvvvde53F133ZUvnqpVq/LTTz/Rr18/wFF9s3bt2thsNm6//XYef/xxoqOjnbG0adOGF198kT59+tCtWzdCQ0PLtKRCeSzZkJGRgc1mK6hkg8uK7OoxrtJZAPyotX4zx6zVwGBjeDCwKsf0QUopP6VUO+CC9O8LkVtAQADt27fn2WefZfLkyaxbt84575577uHLL78stAuncuXKudbjagLLXt7VZQMCArjzzjvZunVrkfHY7XaaNm3Kxo0b2bhxI5s3b+bjjz8GYNGiRTzyyCMcOHCAmJgYbDYbo0aN4vXXXyc9PZ3evXvLc4DLWHH6+O8DBgIRSqnvjZ8YYBoQpZQ6BFiNcYB1wBEcHSzvAE+6P2whyq/ExESOHDniHP/hhx+oV6+ec/zvf/87NWrUYMKECd4Ij44dO+Y6CXz+vOOZxH5+frz55pscPnyYOXPmXHMdjRo1IjU1ld27dwOOp239/PPPZGVlceLECe677z5eeOEFLl26RFpaGklJSdx+++2MHDmSli1bSuIvY8W5qudrCq/+FllAezswspRxCVFhXblyhYkTJ3Lx4kUsFgu33norr732Wq42L7/8MuPGjWPy5MlMnDjRo/GNHj2aCRMmEBERgb+/P+PGjSMmJgZwHNHPmTOHIUOGULVqVZo2bVrgOipXrszbb7/Niy++yMWLF8nMzOTRRx+lYcOGPPXUU1y6dAm73c7QoUOpXr06r7/+Ojt37sTf35+mTZvSuXNnT75k05GyzB5W0frLfZWUZfYcidkzpCyzEEKIEiv3iT9zeC8yhxf/rkYhhDC7cp/4hRBCuEYSvxBCmIwkfiGEMBlJ/EIIYTKS+IXwAneXOE5OTiYiIgIoXollYW7lq0qRC3KW2p0c48VAhM/Lvs/DXbLvF/EWKbEsiiJH/EJ4Ud4yy0888YSzRs/UqVOd5YtffvllAMaMGcPatWudyxf0zSFnieUZM2Ywbtw44uLiuPfee1mwYIEHXpXwdRX2iF+I8uLgwYNs2bKFOnXq0Lt3b3bt2kXjxo35/PPP+eqrr/Dz8+PChQslXn9iYiKffPIJaWlp3H///QwaNIhKlSq58RWI8kaO+IXwslatWhEaGoq/vz8tWrQgOTmZG264gcDAQJ555hnWrVtHlSpVSrz+yMhIAgMDCQ4OJiQkhNOnT7sxelEeSeIXwssKKmtssVj47LPP6N69O5s2bWLAgAGAo8ZMVlYW4Khxf/Xq1SLXn/2kr+z1Z9fpF+YliV8IH5SWlsalS5eIjIzkpZde4r///S8A9erV48ABR5HBL774oliJX4i8pI9fCB90+fJlhg4dSkZGBna7nX/84x8ADBgwgCFDhmC1WuncubPPVhkVvq3cl2XOLtAW8M7qXPNzX87peACYlGUumYoWs5Rldi+J2TOkLLMQQogSk8QvhBAmI4lfCCFMRhK/EEKYjCR+IYQwGUn8QghhMpL4hfCC+vXrExUVhdVqpWvXruzatQuA3377jbp16zJ9+nRn29TUVG655RZeeOEFwFF4bd68eV6JW1QMcgOXML2lPwx06/r6N/+wyDZBQUFs3LgRgK1btzJt2jSWL18OQIMGDdi8eTPPPfccAGvWrKFp06ZujVGYmxzxC+Flly5donr16s7xKlWq0KRJE/bt2wc4En/Pnj29FZ6ogOSIXwgvSE9PJyoqioyMDE6dOoXWOtf82NhYVq1aRUhICP7+/tx0002kpKR4KVpR0UjiF8ILcnb17N69m9GjR7Nlyxbn/PDwcF577TVuvPFGevXq5a0wRQUlXT1CeFmbNm1ITU3l7NmzzmmVK1fmr3/9K2+//Tbdu3f3YnSiIpIjfiG8LDExkczMTGrWrJmrO+exxx6jXbt21KxZ04vRiYqoyMSvlFoI9ABOaa1bGNOCgaXArUASoLTW55RSfkACEANcAR7RWu8tm9CFKL+y+/gB7HY7s2bNIiAgIFebZs2a0axZM2+EJyq44hzxvw+8BSzKMe15YLPWeppS6nlj/DmgG9DE+GkLzDV+C+GzinP5pbslJycXOL1Bgwa5+vqz9e/fn/79+wPwzDPPlGlsouIrso9fa/0VkJpncizwgTH8AdA7x/RFWmu71voboIZS6mZ3BSuEEKL0Snpy9yat9Ulj+HfgJmO4LpDzUOaYMU0IIYSPKPXJXa21XSnl8mO8lFIjgBHGOggJCXFpeYvFQkhICNmnwoqzfN42rm7THbLjLk8qWswpKSlYLL57XYMvx1YYibnsBQYGuu1/saSvPEUpdbPW+qTRlXPKmH4cqJ+jXT1jWj5a6/nAfGPU7uqj/fI+Wq84y2e3CXVhGXeraI8x9FXXijkjIyPfiVRfUR4fCSgxe0ZGRgY2m62gRy+6rKSJfzUwGJhm/F6VY/oopdQSHCd1L+ToEhJCCOEDinM558dAOBCilDoG/ANHwtdKqWHAr4Aymq/DcSlnIo7LOYeUQcxCCCFKocjEr7V+qJBZkQW0tQMjSxuUEBVd/fr1ue2227DZbAQEBBAXF8eIESMA2LdvH8uWLeOVV14pdPmlS5eyf/9+pkyZUuxtzp49m6effrrUsc+YMYN///vfBAcHk5mZyYQJE7BarcVevm3btnz++ecEBwfTq1cvVq9eXeqYimvnzp0MHTqUBg0aYLfbCQ4OZunSpW5b/8GDB0lJSSEy0pEev/jiC3755RdGjRrltm24Q/k6uyFEGQj9/oBb13ei1Z1FtslZq+fMmTOMHDmSy5cv8/zzz9OyZUtatmzp1pgA/vnPf7ol8QMMHz6cxx9/nEOHDvHAAw+wb98+/P1dv0jQ1aSfmZlZ6vMzYWFh/Pvf/y6TPv4ffviB/fv3OxN/ly5d6NKli9u3U1pSq0cILwsJCeG1117jvffew263s3PnTgYNGgTA//3f/9GzZ0+6dOlCr169SExMdC534sQJ4uLiuO+++3jzzTed05cvX0737t2Jiopi/PjxZGZmMnXqVOfdwtlHnwW1y8ykryWtAAAPhklEQVTMZMyYMURERBAZGcn8+fO5liZNmhAQEOCsNTR8+HBiYmKIiYlxPlwmNTWVhx56iM6dO/Pss89it9tzLQ+QlZVFfHw8HTt25MEHH2TgwIGsXbsWcHxDmDJlCl27dmXt2rUkJSUxYMAAoqOj6dOnj3OfFLb94hgzZoxzeznj2rlzJ3FxcQwfPpyOHTsyatQoZ/zff/89vXr1wmq10r17dy5evMgbb7zB6tWriYqKYtWqVSxdutT5AJ3k5GT69euH1WpFKcXx48ed2540aRK9evXi3nvvzRVHWZEj/gJkDndUQwx4x3NfQYW53XLLLWRlZXH69Olc0xs3bsynn36KxWLhq6++Yvr06bzzzjuAI/Fs3ryZKlWq0L17dyIjI7nuuutYvXo1K1eupFKlSsTHx7NixQomTJjAe++95/yWcejQoQLbNWvWjN9//9159/CFCxeuGffevXvx9/enVq1ajBo1iuHDhxMWFsbx48d5+OGH2bZtGzNnziQsLIyxY8eyadMmPv7443zrWbduHceOHWPr1q2cOXOG8PBw553KADVr1mTDhg0AKKWYNm0aDRs2ZO/evcTHx/PJJ5/w4osvFrj9vL777jsiIiKw2+306NGD0aNHX/M1Hjx4kC1btlCnTh1iY2PZtWsXrVq14oknnmDu3Lm0atWKS5cuUaVKFZ599tlcXXA5u5EmTpxIv379UEqxZMkSJk2axMKFCwHHJcYrV64kMTGRIUOG0KNHj2vGVFqS+IXwYRcvXmTMmDEcPXoUPz8/rl696px3//33ExwcDEC3bt347rvvsFgsHDhwgJiYGMBRE6ig676//vrrAttFRUXx22+/MXHiRCIjI+nUqVOBcb3zzjssX76c66+/nvnz5+Pn58f27dv55ZdfnG0uX75MWloa33zzDe+++y4AVquVGjVq5Fvfd999R48ePfD396d27dq0b98+1/zs0tRpaWns2bOHxx57zDnvzz//BCh0+1WrVs21Lle7elq1auW8bLJ58+YkJydTrVo1ateuTatWrQCoVq1akevZs2ePcz/07duXyZMnO+dFR0fj7+9P06ZN8334lwVJ/EL4gF9//RV/f39uvPFGfvrpJ+f0119/nfbt27NgwQKSk5OJi4tzzvPz88u1Dj8/P+x2O/369SM+Pv6a27tWu40bN7J161Y+/PBD1qxZk6sbKVt2Hz/875r4rKws1qxZQ1BQkEuvvTiuu+46wNEldMMNNzi/ueRUmu1bLBaysrKc68n5AVu5cmXncEBAQJmcG8i5jZxdYWVF+viF8LKzZ8/y/PPPM2TIkHzJ/NKlS9SpUwcg31O6tm/fzrlz5/jjjz/YsGED99xzDx06dGDt2rXOm3zOnTvHsWPHAKhUqZIzoRXWLjU1laysLLp378748eM5cKD4J747derEe++95xw/ePAgAO3atePTTz8FYMuWLZw/fz7fsvfccw+fffaZs7vrP//5T4HbqFatGvXr12fNmjWAI0n+8MMP19x+cdSrV8/5Wr/44otcib8gjRo14tSpU3z//feA49uFzWbj+uuv5/LlywUu06ZNG1atctzytGLFCtq29V79SjniF8ILsk+0FnQ5Z05PPPEEY8aMISEhwXmlSLZWrVoxfPhwTp48Sd++fZ1XAo0fP56HHnoIu92OxWJhypQp1KtXjwEDBmC1Wrnzzjt56623CmwXFBTEuHHjnEe/RX1zyOmVV15xXtpps9lo27Yt06dPZ+zYsYwcOZLOnTvTpk0b6tbNX76re/fufP3114SHhxMaGkqLFi244YYbCtzOW2+9RXx8PAkJCdhsNmJjY2nevHmh2y+OAQMGMGTIEKxWK507d3Z+wyhM5cqVmTt3LhMnTiQ9PZ2goCCWLl1K+/btmTNnTq6T6NkmT57M2LFjmTdvHsHBwcycObNYsZUFP098rSgG+4kTJ1xaIPuW/MJOxM6ePds5PDnGcfPwqcavAv+7fK+wy+7K8uRuRSt/4KuuFfOVK1eK/Mf2lvJYSsBdMWf3x6emptKjRw9WrlxJ7dq13RBhfuVxP1+5coUGDRoUVLLBr9CFCiFH/EIInzB48GAuXLjA1atXGT16dJklfSGJXwjhI5YtW+btEExDTu4KIYTJSOIXQgiTkcQvhBAmI4nfAzKH93JeKSR8iz3pEPakQ94OQwiPksQvhJesX7+eunXr5iq8JoQnyFU95VRZ3Gtg1uJ0WVOecev6irv/Vq5cSVhYGCtXruTZZ591awxCXIsc8QvhBWlpaezatYs33njDeRt/VlYWzz33XIGliffv30/fvn2Jjo7m4YcfJiUlxZvhi3JOEr8QXrBhwwbCw8Np1KgRNWvWZP/+/axbt47k5GS2bt3K7Nmz2bNnDwBXr15l4sSJzJ8/n/Xr19O/f/9ilyIQoiDS1SOEF6xcuZJHH30UgNjYWFauXInNZqNnz575ShMfPnyYn3/+mQcffBBwfDOQu1pFaUjiF8LDzp07x44dO/jpp5/w8/MjMzMTPz8/unXrVmB7u91O06ZNnRUphSgt6eoRZaYsLmOtCJfGfvbZZ/Tt25fvvvuOb7/9lt27d9OgQQNq1KjB2rVr85UmbtSoEampqezevRtwdP38/PPP3nwJopyTI34hPGzlypWMHDky17SYmBgOHTpEaGhovtLElStX5u233+bFF1/k4sWLZGZm8uijj9KsWTMvvQJR3lW4xL9maf6HPAhxLf4vzMDv1iYe215BxciGDRsGQEZGBoGBgc7SxLfddhsALVq0YMWKFR6LUVRsFS7xe0NR9f2FKK6//e1vnD9/XkoTizIliV8IN8suAVGSbxGffvppuXtAiCh/5OSuEEKYjCR+YTo+8rhRIVzizvetJH5hOv7+/tKdIspUSkqKW8tq2Gw2/P3dl66lj1+YTlBQEOnp6WRkZGA//AsA/rXrum39WaVYZ2BgIBkZGW6LxRMk5vyOHTsGQLVq1Uq9Lrvdjr+/P0FBQaVeV7YySfxKqWggAQgA3tVaTyuL7QhREn5+flSpUgWAzCVvAxAQ2d1t6y/NOkNCQjhz5ozbYvGEomKePXs2AE8//bSnQipS3pjdHeOOHTsAaN26tVvW525u7+pRSgUAc4BuwB3AQ0qpO9y9nZJY+sNAlv4w0NthCOGS8nK3cnmJU5TNEX8YkKi1PgKglFoCxAL/LYNtOcUu/gmARy11ynIzQpRYWRz5lnSdhT17wRePzvMqDzH6urJI/HWB5Bzjx4C2ZbCdCk1uCnONu5OBJBfvK+rBQLUT4wE41fhVl9fJpzvdss6yiNET/Nx9aZtSKg6I1lo/aowPBNpqrUflaTcCGAGgtfbNjjAhhPB9fq4uUBaXcx4H6ucYr2dMy0VrPV9r3UZr3QZH4C79KKX2lGQ5b/+Ux7glZolbYvaNn0LidllZdPXsApoopf6CI+E/CDxcBtsRQghRAm4/4tda24BRwAbgR8ck/YO7tyOEEKJkyuQ6fq31OmBdWaw7h/llvP6yUh7jlpg9pzzGLTF7jlvidvvJXSGEEL5NavUIIYTJ+HytnqLKPyilAoFFQGvgLNBfa53k6ThzxFPfiOcmwA7M11on5GkTDqwCjhqTVmitX/ZknAVRSiUBl4BMwGZccZVzvh+Ov0UMcAV4RGu919Nx5oinGbA0x6SGwIta61k52oTjA/taKbUQ6AGc0lq3MKYF44j/ViAJUFrrcwUsOxiYaIxO1lp/4MWYXwd6An8Ch4EhWut8j70r6r3k4ZhfAoYDp41mE4zu6LzLeq3UTCFxLwWyn69ZAzivtW5VwLJJuLivfTrx5yj/EIXjRrBdSqnVWuucdwEPA85prRsrpR4EpgP9PR+tkw14Rmu9VylVDdijlNqYJ2aA7VrrHl6IryidtdaFFV7pBjQxftoCc/HizXla65+BVuB8rxwHPi2gqS/s6/eBt3AcFGR7HtistZ6mlHreGH8u50LGh8M/gDY4DiT2GP8D+T4gPBTzRiBea21TSk0H4vPGnMO13ktl5X3yxwwwU2v9RmELFTPXlKX3yRO31tqZx5RSM4AL11jepX3t6109zvIPWus/gezyDznFAtlHQMuASOPI1Cu01iezj4K11pdwXNnkvtKP3hULLNJa27XW3wA1lFI3ezsoQyRwWGv9q7cDKYjW+isgNc/knO/dD4DeBSzaFdiotU41kv1GILrMAs2hoJi11l8YV+4BfIPjPh2fUch+Lo7i5Joyc624jXymgI/dtT1fT/wFlX/Im0SdbYw35AWglkeiK4JS6lbgLuDbAmbfq5Tap5T6XCnV3LORFcoOfKGU2mPcWZ1Xcf4e3vIghf9j+OK+BrhJa33SGP4dR/dgXr68z4cCnxcyr6j3kqeNUkrtV0otVErVLGC+L+/n+4EUrfWhQua7vK99PfGXW0qp64HlwBit9cU8s/cCt2itWwL/BFZ6Or5CdNBa342jS2ekUqqjtwMqDqVUZaAX8EkBs311X+eitbbj+AcuF5RSL+Do1lxcSBNfei/NBRrh6BY8CczwYiwl8RDXPtp3eV/7euIvTvkHZxullAWojuMkr9copSrhSPqLtdYr8s7XWl/UWl82htcBlZRSIR4OMx+t9XHj9ykcfeVheZoUqxyHF3QD9mqt8z3yyFf3tSElu6vM+H2qgDY+t8+VUo/gOBE5wPjAyqcY7yWP0VqnaK0ztdZZwDuFxOJz+xmcOe0Bcl/EkEtJ9rVPn9yleOUfVgODgf8AccCWwt6MnmD0xy0AftRav1lImzo4vrrZlVJhOD6Avf1hVRXw11pfMoa7AHmvflmN4yvzEhwndS/k6KrwpkKPiHxxX+eQ/d6dZvxeVUCbDcDUHN0TXXCcUPUK48qX8UAnrfWVQtoU573kMUqpm3O8T/sABwto5qulZqzAT1rrYwXNLOm+9vkbuJRSMcAsHJdYLdRaT1FKvQzs1lqvVkoFAR/i6EtPBR7MfhaAl+LtAGwHDgBZxuQJQAMArfU8pdQo4AkcX5X/AMZprQuuE+shSqmG/O+KGAvwb2NfPw7OuP1wXHkQjeNyziFa691eCdhgvNl/AxpqrS8Y03LG7BP7Win1MRAOhAApOK7UWQloHO+NX3FczpmqlGoDPJ6jwu1QHO8hgCla6/e8GHM8EMj/Pjy/0Vo/rpQKxXEJZExh7yUvxhyOo5vHjuOy2ce01idzxmwsmy/XeCLmwuLWWi9QSr2PYx/Py9G21Pva5xO/EEII9/L1Pn4hhBBuJolfCCFMRhK/EEKYjCR+IYQwGUn8QghhMpL4hRDCZCTxCyGEyUjiF0IIk/l/F7jxytuojh0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFYhJREFUeJzt3X20XXV95/F3khNIAgjKmSKX8DQITiEjgwlgq4MsHtZgSwld4LcgKGDkgoJFsOXJmbFaEAVqZHWhJQgKDBW+WFuyFHEo4LCsQHnQLhRmaHgICQl5kKeWkISbnPnjHNJAA/dkn4d77+++X2vdlbP32Xv/vt97k8/d+Z199pnQaDSQJJVr4kgXIEnqLYNekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVLjaSBfQ4ttzJamaCcNtMFqCniVLllTar16vs3Llyi5XM7rZ8/hgz+NDJz0PDAy0tZ1TN5JUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVLhR887Yqpb94e+OdAkATLp6/kiXIEmbNGzQR8S1wJHA8syc0Vp3GfAHwFrgCeCUzHyx9dwFwBxgHfDHmfmTHtUuSWpDO1M33wWOeNO6O4AZmfk+4HHgAoCI2Bs4Dtintc83I2JS16qVJG22YYM+M+8Bnn/Tuv+dmUOtxfuA6a3Hs4GbMnNNZj4FLAAO6GK9kqTN1I0XYz8J/Lj1eCdg0UbPLW6tkySNkI5ejI2ILwBDwI0V9h0EBgEyk3q9XqmGZZX26r6q9VdRq9X6Ot5oYM/jgz33aIyqO0bEyTRfpD00M1//4JBngZ032mx6a92/k5nzgHmtxcZYvwd1P+v3nt3jgz2PD/24H32loI+II4BzgQ9n5qqNnpoP/HVEfB0YAPYE/rHKGJKk7mjn8srvAQcD9YhYDHyR5lU2WwJ3RATAfZl5emb+OiISeJTmlM4ZmbmuV8VLkoY3bNBn5vGbWH3N22x/MXBxJ0VJkrrHWyBIUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuFqw20QEdcCRwLLM3NGa927gJuB3YCngcjMFyJiAnAF8HvAKuDkzHy4N6VLktrRzhn9d4Ej3rTufODOzNwTuLO1DPARYM/W1yDwre6UKUmqatigz8x7gOfftHo2cF3r8XXA0Rutvz4zG5l5H7BdROzYrWIlSZtv2Kmbt7BDZi5tPX4O2KH1eCdg0UbbLW6tW8qbRMQgzbN+MpN6vV6pkGWV9uq+qvVXUavV+jreaGDP44M992iMTg+QmY2IaFTYbx4wr7XYWLlyZaeljKh+1l+v1/s63mhgz+ODPW+egYGBtraretXNstenZFp/Lm+tfxbYeaPtprfWSZJGSNUz+vnAScBXW3/eutH6MyPiJuBA4KWNpngkSSOgncsrvwccDNQjYjHwRZoBnxExB1gIRGvz22heWrmA5uWVp/SgZknSZhg26DPz+Ld46tBNbNsAzui0KElS9/jOWEkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuFqnewcEWcDnwIawCPAKcCOwE3A9sBDwMczc22HdUqSKqp8Rh8ROwF/DMzKzBnAJOA44GvA3Mx8D/ACMKcbhUqSqul06qYGTI2IGjANWAocAny/9fx1wNEdjiFJ6kDloM/MZ4HLgWdoBvxLNKdqXszModZmi4GdOi1SklRd5Tn6iHgnMBvYHXgRuAU4YjP2HwQGATKTer1eqY5llfbqvqr1V1Gr1fo63mhgz+ODPfdojA72PQx4KjNXAETED4APAttFRK11Vj8deHZTO2fmPGBea7GxcuXKDkoZef2sv16v93W80cCexwd73jwDAwNtbddJ0D8DfCAipgGvAocCDwJ3A8fSvPLmJODWDsaQJHWokzn6+2m+6PowzUsrJ9I8Qz8POCciFtC8xPKaLtQpSaqoo+voM/OLwBfftPpJ4IBOjitJ6h7fGStJhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCtfRbYolaaxad+pRI11C09/+vOdDeEYvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAd3QIhIrYDvg3MABrAJ4H/B9wM7AY8DURmvtBRlZKkyjo9o78CuD0z/xOwL/AYcD5wZ2buCdzZWpYkjZDKQR8R2wIHAdcAZObazHwRmA1c19rsOuDoTouUJFXXydTN7sAK4DsRsS/wEHAWsENmLm1t8xywQ2clSpI60UnQ14D3A5/NzPsj4greNE2TmY2IaGxq54gYBAZb21Gv1ysVsazSXt1Xtf4qarVaX8cbDex5fOhnz6MlO/rRcydBvxhYnJn3t5a/TzPol0XEjpm5NCJ2BJZvaufMnAfMay02Vq5c2UEpI6+f9dfr9b6ONxrY8/gwHnseGhqq3PPAwEBb21Weo8/M54BFEfHe1qpDgUeB+cBJrXUnAbdWHUOS1LlOP2Hqs8CNEbEF8CRwCs1fHhkRc4CFQHQ4hiSpAx0FfWb+Epi1iacO7eS4kqTu8Z2xklQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klS4WqcHiIhJwIPAs5l5ZETsDtwEbA88BHw8M9d2Oo4kqZpunNGfBTy20fLXgLmZ+R7gBWBOF8aQJFXUUdBHxHTg94Fvt5YnAIcA329tch1wdCdjSJI60+nUzTeAc4FtWsvbAy9m5lBreTGw06Z2jIhBYBAgM6nX65UKWFZpr+6rWn8VtVqtr+ONBvY8PvSz59GSHf3ouXLQR8SRwPLMfCgiDt7c/TNzHjCvtdhYuXJl1VJGhX7WX6/X+zreaGDP48N47HloaKhyzwMDA21t18nUzQeBoyLiaZovvh4CXAFsFxGv/wKZDjzbwRiSpA5VDvrMvCAzp2fmbsBxwF2ZeQJwN3Bsa7OTgFs7rlKSVFkvrqM/DzgnIhbQnLO/pgdjSJLa1PF19ACZ+VPgp63HTwIHdOO4kqTO+c5YSSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4WpVd4yInYHrgR2ABjAvM6+IiHcBNwO7AU8DkZkvdF6qJKmKTs7oh4DPZ+bewAeAMyJib+B84M7M3BO4s7UsSRohlYM+M5dm5sOtx/8CPAbsBMwGrmttdh1wdKdFSpKq68ocfUTsBuwH3A/skJlLW089R3NqR5I0QirP0b8uIrYG/gb4XGa+HBEbnsvMRkQ03mK/QWCwtR31er3S+Msq7dV9Veuvolar9XW80cCex4d+9jxasqMfPXcU9BExmWbI35iZP2itXhYRO2bm0ojYEVi+qX0zcx4wr7XYWLlyZSeljLh+1l+v1/s63mhgz+PDeOx5aGiocs8DAwNtbVd56iYiJgDXAI9l5tc3emo+cFLr8UnArVXHkCR1rpMz+g8CHwceiYhfttZdCHwVyIiYAywE4i32lyT1QeWgz8yfARPe4ulDqx5XktRdvjNWkgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFa7jWyBIKl+j0WD16tWsX7+eCRPe6qrqzi1btow1a9b07PgbW3/caX0ZZzjD9dxoNJg4cSJTpkyp/L036CUNa/Xq1UyePJlarbeRUavVmDRpUk/HeF1jj736Ms5wJm+55bA9Dw0NsXr1aqZOnVppDKduJA1r/fr1PQ95vbVarcb69esr72/QSxpWL6dr1J5OfgYGvaQxYeedd+bwww/nkEMOYXBwkFdffXWkS2rb7NPOGNHx/b+YpM227tSjunq8SVfPH3abKVOmcMcddwBw5plncv3113Paaf/2gmqj0djwwuVoc+tVV47o+Aa9pDHngAMO4LHHHmPRokV87GMfY7/99uORRx7hhhtu4IknnuDyyy9n7dq17LrrrsydO5etttqKO++8ky996UtMmzaN/fffn4X/91Guu+yr/MU132HJsuUsXLKEJcuWMyeOZc5HjwFgzvlfYMnyFaxZs5ZPxjGcOPsPANjrsCOY89Fj+fuf38uULbbg2q9dzH9417tY8fzznH/Z13lmSfND9i75k7OZ9Z9nsNdhR/D4398OwLduvIkf3nU3a157jSMO+q9c8NVLWbVqFaeddhpLly5l/fr1nHXWWcyePbtr3y+DXtKYMjQ0xN13383BBx8MwFNPPcU3vvENZs6cyfPPP88VV1zBzTffzLRp07jyyiuZN28en/70pznvvPP4wQ9+wC677MJnPvOZNxxzwcJnyL+cyyurVnHQ8Z/gE384m8m1GpdfeB7vfMc7eHXNGo6ccxq/f/BBvHPbbVn16mrev8/enHfap7joyr/ir+f/kLNO/gT/c+5f8jv/ZV+uueQi1q1bxytvml76P/c/wFOLF/PDb/8VjUaDU867kHvvvZfly5fz7ne/mxtuuAGAl19+uavfM4Ne0piwevVqDj/8cAAOPPBAjj/+eJYtW8b06dOZOXMmAA899BCPP/74hrPh1157jZkzZ7JgwQJ23XVXdtllFwCOPvpo/tfVV2049qG/+wG23GILttxiC+rv3I4Vzz/PwG/9Ftfe8jfcfs/PAFiyfAVPLlrMzG23ZYvJkznsg78DwPveuxf3PPAgAP/w8MN8439cAMCkSZN4x9Zbv6GHex54gHv+8QH+28mfAuCVV1/lySefZNasWXz5y1/m4osv5rDDDuPAAw/s6vfOoJc0Jmw8R7+xadOmbXjcaDQ46KCD+OY3v/mGbX71q1+97bG3mDx5w+NJEyeybt06fv7wL/jZgw8x/6ormTplCseeeRZr1q4FoFabtOEqmImTmtu3o9GAMz9+Aice/W+vcUx+z28zNDTE7bffzl133cWll17Khz70Ic4+++y2jtmO0feqhSRVNHPmTB544AGeeuopAFatWsUTTzzBHnvswcKFC1m0aBEA8+cP/+Lvv7zyCttusw1Tp0xhwcKF/OLXjw67z4dmvp8b/rb56anr1q3j5X/91zc8/+ED9uemH/2YV1atAmDpihWsWLGC5557jqlTp3LMMcdw+umn88gjj2xW38PxjF5SMbbffnvmzp3LGWecwdrW2fe5557LHnvswVe+8hVOOOEEpk2bxr777ktjq63f9lgHH3gAN/zdfA7+2CfYY5ed2W+fvYcd/0uf+yznfe0v+N4Pb2PSxIlc8qfnMHPGPhue//CB+/PPCxdyVOtyy62mTuVb136HBQsWcNFFFzFhwgQmT57MJZdc0sF34d+b0Gg0unrAihpLliyptGO3L/Oqqp3Lw7qlXq9X/tT4scqeR9aqVaveMEXSK7VajaGhoZ4c+5VXXmGrrbai0Whw4YUXsvu223DqcR/tyVib4/Wpm+Fs6mcwMDAAb/2Rrht4Ri9pXLjxxhu55ZZbeO2115gxYwYnnvSxkS6pbwx6SePC4OAgg4ODG5YbT//zCFbTX74YK0mFM+glDWuUvJY3rnXyMzDoJQ1r4sSJPXuRVMMbGhrq6B4+ztFLGtaUKVNYvXo1a9as6ekti7fccsv+fcLUE4/3ZZzhTN35P7b9CVNV9SzoI+II4ApgEvDtzPxqr8aS1FsTJkyo/OlGm6Ofl5Suu+mq4Tfqgx3+6KSe99yTqZuImARcCXwE2Bs4PiKGf7eBJKnrejVHfwCwIDOfzMy1wE1A9+65KUlqW6+Cfidg0UbLi1vrJEl9NmIvxkbEIDAIkJmvv5V38/3owS5WNXZU/n6NYfY8PvSt51GUHb3uuVdn9M8CO2+0PL21boPMnJeZszJzFs17NVT6ioiHOtl/LH7Z8/j4sufx8dWFnofVqzP6B4A9I2J3mgF/HDB+biwhSaNIT87oM3MIOBP4CfBYc1X+uhdjSZLeXs/m6DPzNuC2Xh1/I/P6MMZoY8/jgz2PDz3vebTcj16S1CPe60aSCjdm7nUz3C0VImJL4HpgJvAb4I8y8+l+19lNbfR8DvApYAhYAXwyMxf2vdAuavfWGRFxDPB9YP/MHD3XyVXQTs8REcCfAQ3gnzJzTF/c0Mbf7V2A64DtWtuc35oOHpMi4lrgSGB5Zs7YxPMTaH4/fg9YBZycmQ93a/wxcUbf5i0V5gAvZOZ7gLnA1/pbZXe12fMvgFmZ+T6aoXdpf6vsrnZvnRER2wBnAff3t8Lua6fniNgTuAD4YGbuA3yu74V2UZs/5/9O8yKO/WhetffN/lbZdd8Fjnib5z8C7Nn6GgS+1c3Bx0TQ094tFWbTPAOAZugd2votOVYN23Nm3p2Zq1qL99F8v8JY1u6tM/6c5i/y1f0srkfa6flU4MrMfAEgM5f3ucZua6fnBvCO1uNtgWofKj1KZOY9wPNvs8ls4PrMbGTmfcB2EbFjt8YfK0Hfzi0VNmzTurzzJWD7vlTXG5t7G4k5wI97WlHvDdtzRLwf2Dkzf9TPwnqonZ/zXsBeEfEPEXFfa9pjLGun5z8DToyIxTSv3vtsf0obMT29bcxYCXq9jYg4EZgFXDbStfRSREwEvg58fqRr6bMazf/SHwwcD1wdEduNaEW9dzzw3cycTnPe+obWz18VjJVv3LC3VNh4m4io0fzv3m/6Ul1vtNMzEXEY8AXgqMzszyc29M5wPW8DzAB+GhFPAx8A5kfErL5V2H3t/JwXA/Mz87XMfAp4nGbwj1Xt9DwHSIDMvBeYAtT7Ut3IaOvfe1Vj5aqbdm6pMB84CbgXOBa4KzPH8psEhu05IvYDrgKOKGDeFobpOTNfYqN/7BHxU+BPxvhVN+383f47mme434mIOs2pnCf7WmV3tdPzM8ChwHcj4rdpBv2KvlbZX/OBMyPiJuBA4KXMXNqtg4+JM/q3uqVCRHw5Io5qbXYNsH1ELADOAc4fmWq7o82eLwO2Bm6JiF9GxPwRKrcr2uy5KG32/BPgNxHxKHA38KeZOWb/t9pmz58HTo2IfwK+R/NywzF74hYR36N5EvreiFgcEXMi4vSIOL21yW00f3kvAK4GPtPN8X1nrCQVbkyc0UuSqjPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkq3P8H1YqhLTZ3OgYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "plt.hist(diabetes_X_train, label=diabetes_features,bins='auto')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(diabetes_y_train, label=diabetes_features, bins='auto')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(diabetes_X_test,label=diabetes_features, bins='auto')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(diabetes_y_test, label=diabetes_features,bins='auto')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly explain why the features should be standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: the features is bias, in real world, if data is big, it should follow normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are standardizing the data (by subtracting off each feature's mean and dividing by its standard deviation) and also doing cross-validation. Explain why standardization should *not* be applied to the full training dataset first before running cross-validation. When should standardization be run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: Remove one or more data locations, and then use data from other locations to predict the data associated with them. In this way, you can compare the predicted values with the measured values and get useful information. If the prediction error is unbiased, the average prediction error should be close to 0. However, this value depends on the size of the data; to standardize it, the standardized prediction error can be obtained by dividing the prediction error by its prediction standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3(b): Support Vector Classification\n",
    "\n",
    "We now train a linear SVM (scikit-learn's `LinearSVC`) to classify whether a Pima Indian woman has diabetes or not. We use 5-fold cross-validation, making sure to account for the feature standardization in part (a).\n",
    "\n",
    "### Problem 3(b)-Subpart (i)\n",
    "\n",
    "Suppose that we sweep over hyperparameter `C` for SVM, using exactly 10 different possible values for `C` with 5-fold cross-validation. How many SVM classifiers will get trained during cross-validation, and (roughly) how many training data get used for each fold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3(b)-Subpart (ii)\n",
    "\n",
    "We shall use 5-fold cross-validation but with a twist! For each fold, compute *two* different scores: the true positive rate and the false positive rate. In other words, after running 5-fold cross-validation, for each of the five hyperparameter choices for `C`, we will have two different numbers: average true positive rate across folds for that hyperparameter choice, and average false positive rate across folds for that hyperparameter choice.\n",
    "\n",
    "Importantly, please use scikit-learn's `LinearSVC` classifier specifying only `C` and `random_state=95865`.\n",
    "\n",
    "**Note**: For this variant of cross-validation, we are not yet selecting a best score yet (especially since there are two scores). This is why in the code provided below, there is no need to keep track of the best score seen so far, etc. However, we make sure to store all the cross-validation scores computed. Note that your code should *not* be using `decision_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:   0.0001 TPR: -1.0000 FPR: -1.0000\n",
      "C:   0.0004 TPR: -1.0000 FPR: -1.0000\n",
      "C:   0.0013 TPR: -1.0000 FPR: -1.0000\n",
      "C:   0.0046 TPR: -1.0000 FPR: -1.0000\n",
      "C:   0.0167 TPR: -1.0000 FPR: -1.0000\n",
      "C:   0.0599 TPR: -1.0000 FPR: -1.0000\n",
      "C:   0.2154 TPR: -1.0000 FPR: -1.0000\n",
      "C:   0.7743 TPR: -1.0000 FPR: -1.0000\n",
      "C:   2.7826 TPR: -1.0000 FPR: -1.0000\n",
      "C:  10.0000 TPR: -1.0000 FPR: -1.0000\n"
     ]
    }
   ],
   "source": [
    "# WARNING: YOU SHOULD ONLY HAVE TO WRITE CODE IN THE PART BELOW THAT SAYS \"TODO\"\n",
    "kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "C_range = np.logspace(-4, 1, 10)  # 10 different choices\n",
    "svm_cross_val_true_positive_rates = []\n",
    "svm_cross_val_false_positive_rates = []\n",
    "\n",
    "for C in C_range:\n",
    "    # --------------------------------------------------------------------------\n",
    "    # TODO: WRITE YOUR CODE HERE\n",
    "    #\n",
    "    cross_val_true_pos_rate = -1  # this value is obviously wrong and is for\n",
    "                                  # you to fill in with the correct 5-fold\n",
    "                                  # cross-validation score for true positive\n",
    "                                  # rate\n",
    "    cross_val_false_pos_rate = -1  # this value is obviously wrong and is for\n",
    "                                   # you to fill in with the correct 5-fold\n",
    "                                   # cross-validation score for false positive\n",
    "                                   # rate\n",
    "    for k, (train, val) in enumerate(kf.split(diabetes_X_train, diabetes_y_train)):\n",
    "        clf = LinearSVC(random_state=95865)\n",
    "        probas_ =clf.fit(diabetes_X_train[train], diabetes_y_train[train])\n",
    "        ypred = clf.predict(diabetes_X_train[val])\n",
    "        yval = diabetes_y_train[val]\n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    print('C: %8.4f' % C,\n",
    "          'TPR: %6.4f' % cross_val_true_pos_rate,\n",
    "          'FPR: %6.4f' % cross_val_false_pos_rate)\n",
    "    svm_cross_val_true_positive_rates.append(cross_val_true_pos_rate)\n",
    "    svm_cross_val_false_positive_rates.append(cross_val_false_pos_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3(b)-Subpart (iii)\n",
    "\n",
    "Plot the true positive rate vs false positive rate. (The x-axis should be FPR and the y-axis should be TPR.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected array-like (array or non-string sequence), got True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-6e82b241e506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrapz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_true_pos_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_false_pos_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_true_pos_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_false_pos_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SVM'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[1;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 534\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \"\"\"\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Check to make sure y_true is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m     if not (y_type == \"binary\" or\n\u001b[1;32m    317\u001b[0m             (y_type == \"multiclass\" and pos_label is not None)):\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         raise ValueError('Expected array-like (array or non-string sequence), '\n\u001b[0;32m--> 244\u001b[0;31m                          'got %r' % y)\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0msparseseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SparseSeries'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected array-like (array or non-string sequence), got True"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(True, 1, pos_label=1)\n",
    "auc = np.trapz(cross_val_true_pos_rate, cross_val_false_pos_rate)\n",
    "plt.plot(cross_val_true_pos_rate, cross_val_false_pos_rate, linestyle='-', alpha=0.5, color='C0',label='SVM' % auc)\n",
    "\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3(b)-Subpart (iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the plot above, suppose that for our clinical application, we can have at most a false positive rate of 12%. Under this constraint, what is the largest true positive rate that we could approximately be able to achieve according to our cross validation results? Which value of hyperparameter `C` (among the ones we tried in cross validation) does this roughly correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important observation**: Your answer to the above question should make it clear that using training data and an ROC curve, we can actually pick a hyperparameter based on some application-specific constraint. The hyperparameter chosen can then be used for test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3(c): Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat problem 3(b) except now using a random forest classifier, sweeping over 4 different `n_estimators` values (50, 100, 150, 200) and 6 `max_depth` values (2, 3, 4, 5, 6, 7), i.e., sweep over a total of 4x6=24 hyperparameter settings. For the `RandomForestClassifier`, aside from `n_estimators` and `max_depth`, only specify one additional setting `random_state=95865`. When you generate your cross-validation-based ROC plot, use a scatter plot (`plt.scatter`) instead of a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9a694712459d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#X_train=pd.Series(diabetes_X_train.reshape(4608, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiabetes_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvoca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(voca)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=50, max_df=0.8,max_features=1000,stop_words=\"english\")\n",
    "#X_train=pd.Series(diabetes_X_train.reshape(4608, 1))\n",
    "X = vectorizer.fit_transform(diabetes_X_train)\n",
    "voca=vectorizer.get_feature_names()\n",
    "#print(voca)\n",
    "leng=[]\n",
    "for i in voca:\n",
    "    leng.append(i)\n",
    "print(len(leng))\n",
    "\n",
    "flattened_X = X.reshape(len(leng), -1).astype(np.float32).todense()\n",
    "num_X = len(flattened_X)\n",
    "indices = range(num_X )\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "num_trees_range = [50, 100, 150, 200]\n",
    "max_depth_range = [2, 3, 4, 5, 6, 7]\n",
    "hyperparam_range = [(num_trees, max_depth)\n",
    "                    for num_trees in num_trees_range\n",
    "                    for max_depth in max_depth_range]\n",
    "rf_cross_val_true_positive_rates = []\n",
    "rf_cross_val_false_positive_rates = []\n",
    "\n",
    "for num_trees, max_depth in hyperparam_range:\n",
    "    # --------------------------------------------------------------------------\n",
    "    # TODO: WRITE YOUR CODE HERE\n",
    "    #\n",
    "    cross_val_true_pos_rate = -1  # this value is obviously wrong and is for\n",
    "                                  # you to fill in with the correct 5-fold\n",
    "                                  # cross-validation score for true positive\n",
    "                                  # rate\n",
    "    cross_val_false_pos_rate = -1  # this value is obviously wrong and is for\n",
    "                                   # you to fill in with the correct 5-fold\n",
    "                                   # cross-validation score for false positive\n",
    "      \n",
    "    # rate\n",
    "    for train_indices, val_indices in kf.split(indices):\n",
    "        rf_classifier = RandomForestClassifier(n_estimators=num_trees, max_depth=max_depth, random_state=95865)  # n_estimators is the number of trees\n",
    "        rf_classifier.fit(flattened_X[train_indices],\n",
    "                       Y_train[train_indices])\n",
    "        rf_predicted_train_labels = rf_classifier.predict(flattened_X[val_indices])\n",
    "        \n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    print('num_trees: %d' % num_trees,\n",
    "          'max_depth: %d' % max_depth,\n",
    "          'TPR: %6.4f' % cross_val_true_pos_rate,\n",
    "          'FPR: %6.4f' % cross_val_false_pos_rate)\n",
    "    rf_cross_val_true_positive_rates.append(cross_val_true_pos_rate)\n",
    "    rf_cross_val_false_positive_rates.append(cross_val_false_pos_rate)\n",
    "\n",
    "    \n",
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR PLOTTING CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the same constraint as in part (b), namely that we want a false positive rate of no greater than 12%, what is the best achievable true positive rate for random forests at least according to the cross-validation results? What hyperparameter setting achieves this? Is this TPR higher than what was possible with a linear SVM classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer (no code)**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3(d): How good is cross-validation in this case anyways?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we focus on using the linear SVM classifier, and we examine how good cross-validation from part (b) is in identifying which hyperparameters are good.\n",
    "\n",
    "### Problem 3(d)-Subpart (i)\n",
    "\n",
    "We start with a basic task: using the best hyperparameter `C` found in part (b), train a linear SVM using the full training dataset. Using this classifier, compute and print out its test set true positive rate and false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3(d)-Subpart (ii)\n",
    "\n",
    "Your code from subpart (i) uses the best choice of hyperparameter `C` from part (b) and computes a test set TPR and a test set FPR. However, you can actually repeat this calculation for each `C` within the range of values from part (b). By doing so, we again get a sequence of TPR's and FPR's from which we can plot an ROC curve! Put another way, instead of plotting an ROC curve using cross-validation, we can plot an ROC curve using simple data splitting, where the training dataset is the actual full training dataset, and the validation data is the actual test set. Plot the resulting ROC curve in this case, and overlay your training cross-validation ROC from earlier.\n",
    "\n",
    "**Disclaimer**: At this point, we are definitely looking at the test data, so this analysis is meant for better understanding what we did right or wrong during the training phase in which we didn't peek at the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THE NEXT TWO LINES; THESE ARE FOR YOU TO FILL OUT AND PLOT\n",
    "svm_test_true_positive_rates = []\n",
    "svm_test_false_positive_rates = []\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# TODO: WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your SVM cross-validation ROC curve from part (b) accurately estimate the true ROC curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
